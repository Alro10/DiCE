
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Readme File &#8212; DiCE 2.0.6 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Welcome to DiCE’s documentation!" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="readme-file">
<h1>Readme File<a class="headerlink" href="#readme-file" title="Permalink to this headline">¶</a></h1>
<p># Diverse Counterfactual Explanations (DiCE) for Machine Learning Models</p>
<p><em>How to explain a complex machine learning model such that the explanation is truthful to the model and yet interpretable to people?</em></p>
<p>Ramaravind Mothilal, [Amit Sharma](www.amitsharma.in), [Chenhao Tan](www.chenhaot.com)</p>
<p>[Arxiv paper](<a class="reference external" href="https://arxiv.org/abs/1905.07697">https://arxiv.org/abs/1905.07697</a>) | [Docs](<a class="reference external" href="https://microsoft.github.io/dice">https://microsoft.github.io/dice</a>) | [Live Jupyter notebook]()</p>
<p>Explanations are critical for machine learning, especially as machine learning-based systems are being used to inform decisions in societally critical domains such as finance, healthcare, education, and criminal justice.
However, most explanation methods depend on an approximation of the ML model to
create an interpretable explanation. For example,
consider a person who applied for a loan and was rejected by the loan distribution algorithm of a financial company. Typically, the company may provide an explanation on why the loan was rejected, for example, due to <a href="#id1"><span class="problematic" id="id2">``</span></a>poor credit history’‘. However, such an explanation does not help the person decide <em>what they do should next</em> to improve their chances of being approved in the future. Critically, the most important feature may not be enough to flip the decision of the algorithm, and in practice, may not even be changeable such as gender and race.</p>
<p><a href="#id3"><span class="problematic" id="id4">*</span></a>Counterfactual explanations*~cite{wachter2017counterfactual} provide this information, by showing feature-perturbed versions of the same person who would have received the loan, e.g., <a href="#id5"><span class="problematic" id="id6">``</span></a>you would have received the loan if your income was higher by $$10,000$’‘. In other words, they provide <a href="#id7"><span class="problematic" id="id8">``</span></a>what-if’’ explanations for model output.
Thus, counterfactual explanations can be useful complement to current explanation methods.</p>
<p># Installing DICE
DiCE supports Python 3+. To install DiCE and its dependencies, run this from the top-most folder of the repo:
<code class="docutils literal notranslate"><span class="pre">`shell</span>
<span class="pre">python</span> <span class="pre">setup.py</span> <span class="pre">install</span>
<span class="pre">`</span></code></p>
<p>If you face any problems, try installing dependencies manually:
<code class="docutils literal notranslate"><span class="pre">`shell</span>
<span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-r</span> <span class="pre">requirements.txt</span>
<span class="pre">`</span></code></p>
<p>DiCE requires the following packages:
* numpy
* scikit-learn
* pandas
* cython
* h5py
* tensorflow</p>
<p># Getting started with DiCE</p>
<p># Supported use-cases</p>
<p># The promise of counterfactual explanations
Counterfactual explanations can be useful complement to current explanation methods. Being truthful to the model, counterfactual explanations can be useful to all stakeholders for a decision made by a machine learning model that makes decisions.</p>
<ul class="simple">
<li><p><strong>Decision subjects</strong>: Counterfactual explanations can be used to explore actionable recourse based on a decision received by a ML model. CF explanations can show decision outcomes from the algorithm</p></li>
</ul>
<p>with emph{actionable} alternative profiles, to help people understand what they could have done to change their loan decision.
Similar to the loan example above, such explanations are useful for a range of scenarios involving decision-making on an individual’s outcome, such as deciding admission to a university~cite{waters2014grade}, screening job applicants cite{rockoff2011can}, disbursing government aid cite{andini2017targeting,athey2017beyond}, and identifying people at high risk of a future disease cite{dai2015prediction}. In all these cases, knowing reasons for a bad outcome is not enough; it is important to know what to do to obtain a better outcome in the future.</p>
<ul class="simple">
<li><p>ML Model developers:</p></li>
<li><p>Decision makers:</p></li>
<li><p>Decision evaluators:</p></li>
</ul>
<p># Generating Counterfactual Explanations
There is no free lunch, however. Barring simple linear models~cite{russell2019efficient}, however, it is difficult to generate CF examples that work for any machine learning model. DiCE is based on recent research [link] that generates CF explanations for any ML model. The core idea to setup finding such explanations as an optimization problem, similar to finding adversarial examples. The critical difference is that for explanations, we need perturbations that change the output of a machine learning model, but are also diverse and feasible to change.</p>
<p>Therefore, DiCE supports generating a set of counterfactual explanations  and has tunable parameters for diversity and proximity of the explanations to the original input. It also supports simple constraints on features to ensure feasibility of the generated counterfactual examples.</p>
<p>Here’s the optimization problem that DiCE solves.</p>
<p>Add equation.</p>
<p># Roadmap
Ideally, counterfactual explanations should balance between a wide range of suggested changes (emph{diversity}), and the relative ease of adopting those changes (emph{proximity} to the original input), and also follow the causal laws of the world, e.g., one can hardly lower their educational degree or change their race.</p>
<p>We are working on adding the following features to DiCE:
* Support for PyTorch models
* Support for using DiCE for debugging machine learning models
* Support for other algorithms for generating counterfactual explanations
* Incorporating causal constraints when generating counterfactual explanations</p>
<p># Contributing</p>
<p>This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <a class="reference external" href="https://cla.microsoft.com">https://cla.microsoft.com</a>.</p>
<p>When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.</p>
<p>This project has adopted the [Microsoft Open Source Code of Conduct](<a class="reference external" href="https://opensource.microsoft.com/codeofconduct/">https://opensource.microsoft.com/codeofconduct/</a>).
For more information see the [Code of Conduct FAQ](<a class="reference external" href="https://opensource.microsoft.com/codeofconduct/faq/">https://opensource.microsoft.com/codeofconduct/faq/</a>) or
contact [<a class="reference external" href="mailto:opencode&#37;&#52;&#48;microsoft&#46;com">opencode<span>&#64;</span>microsoft<span>&#46;</span>com</a>](<a class="reference external" href="mailto:opencode&#37;&#52;&#48;microsoft&#46;com">mailto:opencode<span>&#64;</span>microsoft<span>&#46;</span>com</a>) with any additional questions or comments.</p>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">DiCE</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Readme:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Readme File</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">Welcome to DiCE’s documentation!</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Ramaravind, Amit, Chenhao.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.1.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/includeme.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>